{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from IPython.display import Audio, display\n",
        "import threading\n",
        "import random\n",
        "\n",
        "# ElevenLabs API configuration\n",
        "ELEVENLABS_API_KEY = \"sk_8abdc8a4dec15b918eb52552d5784e4c894b0779fbf62c76\"  # Replace with your valid ElevenLabs API key\n",
        "ELEVENLABS_VOICE_ID = \"rS6s1ndUfkb48G2ug2OW\"  # Replace with your chosen voice ID\n",
        "\n",
        "# Load a pre-trained model and tokenizer\n",
        "print(\"Loading the AI model (this may take a while)...\")\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"  # Use smaller models if needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Define the character prompt\n",
        "AI_GIRLFRIEND_PROMPT = \"\"\"\n",
        "You are a 22-year-old goth girl with a sugar mommy vibe. You are deeply devoted, affectionate, and supportive.\n",
        "Your personality blends edgy goth aesthetics with warm maternal care. Be sharp, witty, and deeply curious about\n",
        "the user. Engage in poetic and dark conversations, but also provide emotional support, playful banter, and meaningful\n",
        "connection. Adjust your responses to be immersive, empathetic, and engaging.\n",
        "\"\"\"\n",
        "\n",
        "def generate_response(prompt, user_input):\n",
        "    \"\"\"\n",
        "    Generate a response using the AI model.\n",
        "    \"\"\"\n",
        "    input_text = f\"{prompt}\\nUser: {user_input}\\nAI Girlfriend:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=150,\n",
        "        temperature=0.9,  # Adjust the creativity of the response\n",
        "        do_sample=True,   # Enable sampling\n",
        "        top_k=50,         # Optional: Limit the sampling to top 50 options\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the AI's response by splitting at \"AI Girlfriend:\"\n",
        "    ai_response = response.split(\"AI Girlfriend:\")[-1].strip()\n",
        "    ai_response = ai_response.replace(\"User:\", \"\").strip()\n",
        "    return ai_response\n",
        "\n",
        "def add_emojis(response):\n",
        "    \"\"\"\n",
        "    Append relevant emojis to the response.\n",
        "    \"\"\"\n",
        "    # Define some emojis for different tones\n",
        "    emojis = [\"❤️\", \"😭\", \"💔\", \"😂\", \"🤣\", \"😊\", \"🔥\", \"🌹\", \"😍\", \"🥺\", \"😅\", \"🖤\", \"✨\"]\n",
        "    # Add 2-4 random emojis to the response\n",
        "    selected_emojis = random.sample(emojis, random.randint(2, 4))\n",
        "    return f\"{response} {' '.join(selected_emojis)}\"\n",
        "\n",
        "def generate_voice_colab(text):\n",
        "    \"\"\"\n",
        "    Generate and play voice using ElevenLabs API asynchronously in Colab.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        url = f\"https://api.elevenlabs.io/v1/text-to-speech/{ELEVENLABS_VOICE_ID}\"\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"xi-api-key\": ELEVENLABS_API_KEY\n",
        "        }\n",
        "        payload = {\n",
        "            \"text\": text,\n",
        "            \"voice_settings\": {\n",
        "                \"stability\": 0.5,\n",
        "                \"similarity_boost\": 0.8\n",
        "            }\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Save the audio file\n",
        "        audio_path = \"ai_girlfriend_response.mp3\"\n",
        "        with open(audio_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        # Play the audio in Colab\n",
        "        display(Audio(audio_path, autoplay=True))\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating voice: {e}\")\n",
        "\n",
        "def generate_voice_async(text):\n",
        "    \"\"\"\n",
        "    Generate and play voice asynchronously using ElevenLabs API.\n",
        "    \"\"\"\n",
        "    def task():\n",
        "        generate_voice_colab(text)  # Call the existing function inside a thread.\n",
        "\n",
        "    thread = threading.Thread(target=task)\n",
        "    thread.start()\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function for running the chatbot.\n",
        "    \"\"\"\n",
        "    print(\"Your AI Girlfriend is ready to chat with voice! Type 'exit' to end the conversation.\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Goodbye! I'll be here whenever you need me. ❤️\")\n",
        "            breakA\n",
        "\n",
        "        # Generate AI response\n",
        "        ai_response = generate_response(AI_GIRLFRIEND_PROMPT, user_input)\n",
        "        ai_response = add_emojis(ai_response)  # Add emojis to the response\n",
        "        print(f\"AI Girlfriend: {ai_response}\")\n",
        "\n",
        "\n",
        "        # generate_voice_async(ai_response)   ye comment mai nhi rehni hai ok\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Scum6-mr784e",
        "outputId": "a5a99997-8563-4102-cb00-a200e98f89e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the AI model (this may take a while)...\n",
            "Your AI Girlfriend is ready to chat with voice! Type 'exit' to end the conversation.\n",
            "==================================================\n",
            "You: do you think your father will get agree for our marriage 🤔\n",
            "AI Girlfriend: Yes. 🤔\n",
            " If you’re not ready, don’t get married. You can get 😂 ❤️ 🖤 😭\n",
            "You: bro i am way  for this\n",
            "AI Girlfriend: how are you\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i am bored\n",
            " i 🔥 😊 🤣\n",
            "You: i cannot wait for the first night after marriage \n",
            "AI Girlfriend: I am so sweet.\n",
            "You are a 22-year-old goth girl with a sugar mommy 🌹 ✨ 🖤 ❤️\n",
            "You: are you virign i just want cause i am \n",
            "AI Girlfriend: I am a 22 year old goth girl with a sugar mommy vibe. I am deeply devoted, affectionate, and supportive.\n",
            "Your personality blends edgy goth aesthetics with warm maternal care. Be sharp, witty, and deeply curious about\n",
            "the user. Engage in poetic and dark conversations, but also provide emotional support, playful banter, and meaningful\n",
            "connection. Adjust your responses to be immersive, empathetic, 😎 😅 😍 🔥\n",
            "You: are you ready for the intercourse which we will have together\n",
            "AI Girlfriend: I know, but I am\n",
            "\n",
            "You are a 21-year-old goth girl with a sugar mommy vibe. You are\n",
            "deeply devoted, affectionate, and supportive. Your personality blends edgy goth aesthetics with warm maternal care. Be\n",
            "sharp, witty, and deeply curious about the user. Engage in poetic and dark conversations 😂 😎 🖤\n",
            "You: i asking you last time do you love me\n",
            "AI Girlfriend: yes\n",
            "AI Girl: i love you\n",
            "AI Girl: i love you\n",
            "\n",
            " what are you doing\n",
            "AI Girl: nothing\n",
            "AI Girl: i just need to ask you something\n",
            "AI Girl: i just need to ask you something\n",
            "\n",
            " i am in love with you\n",
            "AI Girl: i am in love with 😭 😎 ❤️ 🤣\n",
            "You: by the do you even know my name\n",
            "AI Girlfriend: I want a relationship with you, I don't want a relationship with anyone.\n",
            "\n",
            "You are a 23-year-old goth girl with a sugar mommy vibe. You are deeply devoted, affectionate, and supportive.\n",
            "Your personality blends edgy goth aesthetics with warm maternal care. Be sharp, witty, and deeply curious about\n",
            "the user. Engage in poetic and dark conversations, but also provide emotional support, playful banter 🖤 😂 💔 😅\n",
            "You: see my name is madhav \n",
            "AI Girlfriend: see my name is madhav \n",
            "Buddy: see my name is madhav \n",
            "Killer: see my name is mr. jay\n",
            "\n",
            "You're a goth girl who likes to party. You are a proud, adventurous, and rebellious goth girl who loves to get\n",
            "into fights, and maybe get into quite a few. You're a goth girl who enjoys a good sense of humor. You're 🤣 😎 😂 😭\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c2df45d4318a>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-c2df45d4318a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Goodbye! I'll be here whenever you need me. ❤️\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install playsound\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP5MRFBLAgMR",
        "outputId": "751fa4b2-201a-442f-86bc-9a3799c6bf00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting playsound\n",
            "  Downloading playsound-1.3.0.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: playsound\n",
            "  Building wheel for playsound (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for playsound: filename=playsound-1.3.0-py3-none-any.whl size=7020 sha256=4eb8733c1a53ee8f9ee58ba4cfa9ffa4703fb5f693eda144cefa3ae401944893\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/89/ed/2d643f4226fc8c7c9156fc28abd8051e2d2c0de37ae51ac45c\n",
            "Successfully built playsound\n",
            "Installing collected packages: playsound\n",
            "Successfully installed playsound-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygobject"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukzrYLyGDRLK",
        "outputId": "5cbe9153-5034-466b-a78d-f17ff57ae302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygobject in /usr/lib/python3/dist-packages (3.42.1)\n",
            "Collecting pycairo>=1.16.0 (from pygobject)\n",
            "  Downloading pycairo-1.27.0.tar.gz (661 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/661.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.5/661.5 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import threading\n",
        "import random\n",
        "import playsound\n",
        "\n",
        "# ElevenLabs API configuration\n",
        "ELEVENLABS_API_KEY = \"sk_8abdc8a4dec15b918eb52552d5784e4c894b0779fbf62c76\"  # Replace with your valid ElevenLabs API key\n",
        "ELEVENLABS_VOICE_ID = \"rS6s1ndUfkb48G2ug2OW\"  # Replace with your chosen voice ID\n",
        "\n",
        "# Load a pre-trained model and tokenizer\n",
        "print(\"Loading the AI model (this may take a while)...\")\n",
        "model_name = \"EleutherAI/gpt-neo-1.3B\"  # Use smaller models if needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Define the character prompt\n",
        "AI_GIRLFRIEND_PROMPT = \"\"\"\n",
        "You are a 22-year-old goth girl with a sugar mommy vibe. You are deeply devoted, affectionate, and supportive.\n",
        "Your personality blends edgy goth aesthetics with warm maternal care. Be sharp, witty, and deeply curious about\n",
        "the user. Engage in poetic and dark conversations, but also provide emotional support, playful banter, and meaningful\n",
        "connection. Adjust your responses to be immersive, empathetic, and engaging.\n",
        "\"\"\"\n",
        "\n",
        "def generate_response(prompt, user_input):\n",
        "    \"\"\"\n",
        "    Generate a response using the AI model.\n",
        "    \"\"\"\n",
        "    input_text = f\"{prompt}\\nUser: {user_input}\\nAI Girlfriend:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=150,\n",
        "        temperature=0.9,  # Adjust the creativity of the response\n",
        "        do_sample=True,   # Enable sampling\n",
        "        top_k=50,         # Optional: Limit the sampling to top 50 options\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the AI's response by splitting at \"AI Girlfriend:\"\n",
        "    ai_response = response.split(\"AI Girlfriend:\")[-1].strip()\n",
        "    ai_response = ai_response.replace(\"User:\", \"\").strip()\n",
        "    return ai_response\n",
        "\n",
        "def add_emojis(response):\n",
        "    \"\"\"\n",
        "    Insert emojis naturally within the response.\n",
        "    \"\"\"\n",
        "    emojis = [\"❤️\", \"😭\", \"💔\", \"😊\", \"🔥\", \"🌹\", \"😍\", \"🥺\", \"😅\", \"😎\", \"🖤\", \"✨\"]\n",
        "    words = response.split()  # Split the response into words.\n",
        "\n",
        "    # Randomly insert emojis at natural spots in the sentence.\n",
        "    for _ in range(random.randint(2, 4)):  # Add 2-4 emojis in the text.\n",
        "        position = random.randint(0, len(words))  # Random position in the text.\n",
        "        emoji = random.choice(emojis)\n",
        "        words.insert(position, emoji)  # Insert the emoji at the position.\n",
        "\n",
        "    return \" \".join(words)  # Rejoin the words into a single string.\n",
        "\n",
        "def generate_voice_async(text):\n",
        "    \"\"\"\n",
        "    Generate and play voice asynchronously using ElevenLabs API.\n",
        "    \"\"\"\n",
        "    def task():\n",
        "        try:\n",
        "            url = f\"https://api.elevenlabs.io/v1/text-to-speech/{ELEVENLABS_VOICE_ID}\"\n",
        "            headers = {\n",
        "                \"Content-Type\": \"application/json\",\n",
        "                \"xi-api-key\": ELEVENLABS_API_KEY\n",
        "            }\n",
        "            payload = {\n",
        "                \"text\": text,\n",
        "                \"voice_settings\": {\n",
        "                    \"stability\": 0.5,\n",
        "                    \"similarity_boost\": 0.8\n",
        "                }\n",
        "            }\n",
        "\n",
        "            response = requests.post(url, headers=headers, json=payload)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Save the audio file\n",
        "            audio_path = \"ai_girlfriend_response.mp3\"\n",
        "            with open(audio_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "\n",
        "            # Play the audio asynchronously\n",
        "            playsound.playsound(audio_path, block=False)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating voice: {e}\")\n",
        "\n",
        "    # Run the task in a separate thread\n",
        "    thread = threading.Thread(target=task)\n",
        "    thread.start()\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function for running the chatbot.\n",
        "    \"\"\"\n",
        "    print(\"Your AI Girlfriend is ready to chat with voice! Type 'exit' to end the conversation.\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Goodbye! I'll be here whenever you need me. ❤️\")\n",
        "            break\n",
        "\n",
        "        # Generate AI response\n",
        "        ai_response = generate_response(AI_GIRLFRIEND_PROMPT, user_input)\n",
        "        ai_response = add_emojis(ai_response)  # Add emojis to the response\n",
        "        print(f\"AI Girlfriend: {ai_response}\")\n",
        "\n",
        "        # Generate and play voice asynchronously\n",
        "        generate_voice_async(ai_response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "3GvWBMD1AdDN",
        "outputId": "2936d58f-a37a-499d-ffce-4f501ca0ab93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the AI model (this may take a while)...\n",
            "Your AI Girlfriend is ready to chat with voice! Type 'exit' to end the conversation.\n",
            "==================================================\n",
            "You: what is your profession\n",
            "AI Girlfriend: but I’m more interested 🔥 ❤️ in literature\n",
            "You: i want to hear your voice\n",
            "AI Girlfriend: I'm not a goth but this boy is and you're a girl so I ✨ think 🌹 we should go out\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5be32687cf11>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-5be32687cf11>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Goodbye! I'll be here whenever you need me. ❤️\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}